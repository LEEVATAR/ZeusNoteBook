{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a283e588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'매우': 1, '나는': 2, '엄청': 3, '잘': 4, '인공지능이': 5, '진짜': 6, '정말': 7, '재미있어서': 8, '시간이': 9, '간다': 10, '긍정적이고': 11, '성격이': 12, '밝으며': 13, '밥도': 14, '먹는다': 15}\n",
      "[[2, 5, 6, 1, 1, 7, 8, 9, 3, 3, 1, 4, 10], [2, 1, 11, 12, 13, 14, 1, 1, 4, 15]]\n",
      "[2, 5, 6, 1, 1, 7, 8, 9, 3, 3, 1, 4, 10, 2, 1, 11, 12, 13, 14, 1, 1, 4, 15]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text1 = '나는 인공지능이 진짜 매우 매우 정말 재미있어서 시간이 엄청 엄청 매우 잘 간다'\n",
    "text2 = '나는 매우 긍정적이고 성격이 밝으며 밥도 매우 매우 잘 먹는다'\n",
    "\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([text1, text2])  # fit하면서 index가 생성됨\n",
    "\n",
    "print(token.word_index) # 빈도가 높은 index가 앞에 프린트되고 다음 순서대로 프린트 됨\n",
    "\n",
    "x = token.texts_to_sequences([text1, text2])\n",
    "print(x) \n",
    "\n",
    "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
    "\n",
    "x_new = x[0] + x[1]\n",
    "print(x_new)\n",
    "\n",
    "\n",
    "# x = to_categorical(x_new)\n",
    "# print(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73beaa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23,)\n",
      "(23, 1)\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(23, 15)\n"
     ]
    }
   ],
   "source": [
    "############################### 원 핫으로 수정 #############################\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "onehot_encoder = OneHotEncoder(categories='auto', sparse=False)\n",
    "x = np.array(x_new)\n",
    "print(x.shape)  \n",
    "x = x.reshape(-1, 1)\n",
    "print(x.shape)  \n",
    "onehot_encoder.fit(x)\n",
    "x = onehot_encoder.transform(x)\n",
    "print(x)\n",
    "print(x.shape)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
