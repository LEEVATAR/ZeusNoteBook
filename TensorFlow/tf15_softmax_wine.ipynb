{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74028e90",
   "metadata": {},
   "source": [
    "# Wine 데이터를 사용하여 다중분류하기\n",
    "\n",
    "### One Hot Encoding의 다른 방법 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e41392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6cd33683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178,)\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터\n",
    "datasets = load_wine()\n",
    "x = datasets.data\n",
    "y = datasets.target\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b42bb5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "(178, 3)\n"
     ]
    }
   ],
   "source": [
    "# One Hot Encoding 01\n",
    "y= to_categorical(y)\n",
    "\n",
    "print(y)\n",
    "print(y.shape)  # (178, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab8206a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding 02\n",
    "# y =pd.get_dummies(y)\n",
    "\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "494f6c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142, 13) (142, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "  x, y, train_size=0.8, shuffle=True, random_state=72\n",
    ")\n",
    "\n",
    "print(x_train.shape, y_train.shape) # (142, 13) (142, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72799ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. 모델구성\n",
    "model = Sequential()\n",
    "model.add(Dense(20, activation='linear',input_dim=13))\n",
    "model.add(Dense(50, activation='relu')) # elu, swish 등등\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(70, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(3,activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab094912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "2/2 [==============================] - 0s 186ms/step - loss: 67.0130 - accuracy: 0.3894 - val_loss: 28.9777 - val_accuracy: 0.2414\n",
      "Epoch 2/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 26.5531 - accuracy: 0.2566 - val_loss: 9.0439 - val_accuracy: 0.3103\n",
      "Epoch 3/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 10.0950 - accuracy: 0.3009 - val_loss: 7.3789 - val_accuracy: 0.6552\n",
      "Epoch 4/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 9.8937 - accuracy: 0.5221 - val_loss: 13.0884 - val_accuracy: 0.4483\n",
      "Epoch 5/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 13.9445 - accuracy: 0.4602 - val_loss: 8.2738 - val_accuracy: 0.4828\n",
      "Epoch 6/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 10.2284 - accuracy: 0.3628 - val_loss: 4.7722 - val_accuracy: 0.5517\n",
      "Epoch 7/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.6711 - accuracy: 0.5044 - val_loss: 3.7861 - val_accuracy: 0.4483\n",
      "Epoch 8/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 3.5896 - accuracy: 0.4602 - val_loss: 8.0530 - val_accuracy: 0.2414\n",
      "Epoch 9/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 7.9584 - accuracy: 0.2832 - val_loss: 3.9437 - val_accuracy: 0.3793\n",
      "Epoch 10/1000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 3.6879 - accuracy: 0.4513 - val_loss: 2.1243 - val_accuracy: 0.6897\n",
      "Epoch 11/1000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 2.1757 - accuracy: 0.6372 - val_loss: 3.4303 - val_accuracy: 0.6207\n",
      "Epoch 12/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.8850 - accuracy: 0.4956 - val_loss: 2.9852 - val_accuracy: 0.6552\n",
      "Epoch 13/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.0324 - accuracy: 0.5310 - val_loss: 2.1384 - val_accuracy: 0.6207\n",
      "Epoch 14/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.9683 - accuracy: 0.6991 - val_loss: 1.2202 - val_accuracy: 0.6207\n",
      "Epoch 15/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.1052 - accuracy: 0.6195 - val_loss: 2.6481 - val_accuracy: 0.2414\n",
      "Epoch 16/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 2.1959 - accuracy: 0.3451 - val_loss: 1.7751 - val_accuracy: 0.6552\n",
      "Epoch 17/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.5827 - accuracy: 0.6903 - val_loss: 2.0465 - val_accuracy: 0.6897\n",
      "Epoch 18/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 2.1776 - accuracy: 0.6549 - val_loss: 1.8442 - val_accuracy: 0.6552\n",
      "Epoch 19/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 2.3417 - accuracy: 0.5841 - val_loss: 1.0753 - val_accuracy: 0.6552\n",
      "Epoch 20/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 1.3106 - accuracy: 0.6106 - val_loss: 0.9817 - val_accuracy: 0.5862\n",
      "Epoch 21/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.8440 - accuracy: 0.6460 - val_loss: 1.8798 - val_accuracy: 0.4828\n",
      "Epoch 22/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.5629 - accuracy: 0.5310 - val_loss: 1.4981 - val_accuracy: 0.4483\n",
      "Epoch 23/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 1.2552 - accuracy: 0.4867 - val_loss: 0.9538 - val_accuracy: 0.6552\n",
      "Epoch 24/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.9138 - accuracy: 0.6814 - val_loss: 1.6932 - val_accuracy: 0.6897\n",
      "Epoch 25/1000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 1.7709 - accuracy: 0.6637 - val_loss: 1.1701 - val_accuracy: 0.6897\n",
      "Epoch 26/1000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 1.2048 - accuracy: 0.6637 - val_loss: 1.2442 - val_accuracy: 0.4828\n",
      "Epoch 27/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 1.1110 - accuracy: 0.5133 - val_loss: 1.0384 - val_accuracy: 0.6897\n",
      "Epoch 28/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.8334 - accuracy: 0.6726 - val_loss: 0.8772 - val_accuracy: 0.6207\n",
      "Epoch 29/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6924 - accuracy: 0.7168 - val_loss: 0.7830 - val_accuracy: 0.6897\n",
      "Epoch 30/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5729 - accuracy: 0.7788 - val_loss: 1.3103 - val_accuracy: 0.6552\n",
      "Epoch 31/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 1.1893 - accuracy: 0.6991 - val_loss: 0.7438 - val_accuracy: 0.6207\n",
      "Epoch 32/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.7878 - accuracy: 0.6195 - val_loss: 0.7258 - val_accuracy: 0.6552\n",
      "Epoch 33/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.5994 - accuracy: 0.7345 - val_loss: 0.7811 - val_accuracy: 0.6552\n",
      "Epoch 34/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5763 - accuracy: 0.7168 - val_loss: 0.9784 - val_accuracy: 0.5517\n",
      "Epoch 35/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.9320 - accuracy: 0.5841 - val_loss: 0.8704 - val_accuracy: 0.6552\n",
      "Epoch 36/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.8634 - accuracy: 0.6814 - val_loss: 0.8149 - val_accuracy: 0.6552\n",
      "Epoch 37/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6987 - accuracy: 0.6903 - val_loss: 0.8852 - val_accuracy: 0.6207\n",
      "Epoch 38/1000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.7365 - accuracy: 0.6814 - val_loss: 0.8948 - val_accuracy: 0.6552\n",
      "Epoch 39/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.6762 - accuracy: 0.6991 - val_loss: 0.7035 - val_accuracy: 0.7931\n",
      "Epoch 40/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6416 - accuracy: 0.7257 - val_loss: 0.6740 - val_accuracy: 0.6207\n",
      "Epoch 41/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5673 - accuracy: 0.7876 - val_loss: 0.9494 - val_accuracy: 0.6552\n",
      "Epoch 42/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.7379 - accuracy: 0.7168 - val_loss: 0.9048 - val_accuracy: 0.5862\n",
      "Epoch 43/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7331 - accuracy: 0.6549 - val_loss: 0.8132 - val_accuracy: 0.6552\n",
      "Epoch 44/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.6082 - accuracy: 0.7080 - val_loss: 0.6988 - val_accuracy: 0.6552\n",
      "Epoch 45/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5740 - accuracy: 0.7699 - val_loss: 0.6823 - val_accuracy: 0.6552\n",
      "Epoch 46/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5508 - accuracy: 0.7788 - val_loss: 0.7071 - val_accuracy: 0.6552\n",
      "Epoch 47/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5528 - accuracy: 0.7257 - val_loss: 0.7604 - val_accuracy: 0.6897\n",
      "Epoch 48/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6545 - accuracy: 0.6991 - val_loss: 0.6903 - val_accuracy: 0.6207\n",
      "Epoch 49/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5186 - accuracy: 0.8230 - val_loss: 0.7614 - val_accuracy: 0.6552\n",
      "Epoch 50/1000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5629 - accuracy: 0.8319 - val_loss: 0.7520 - val_accuracy: 0.6552\n",
      "Epoch 51/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5237 - accuracy: 0.7965 - val_loss: 0.8440 - val_accuracy: 0.5517\n",
      "Epoch 52/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7764 - accuracy: 0.6460 - val_loss: 0.6793 - val_accuracy: 0.7931\n",
      "Epoch 53/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.6355 - accuracy: 0.7345 - val_loss: 0.7519 - val_accuracy: 0.6552\n",
      "Epoch 54/1000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5956 - accuracy: 0.6903 - val_loss: 0.7837 - val_accuracy: 0.7241\n",
      "Epoch 55/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6339 - accuracy: 0.6903 - val_loss: 0.6387 - val_accuracy: 0.6552\n",
      "Epoch 56/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5712 - accuracy: 0.7345 - val_loss: 0.6025 - val_accuracy: 0.6897\n",
      "Epoch 57/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.5281 - accuracy: 0.7611 - val_loss: 0.7362 - val_accuracy: 0.6897\n",
      "Epoch 58/1000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5745 - accuracy: 0.8230 - val_loss: 0.7725 - val_accuracy: 0.7931\n",
      "Epoch 59/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.6120 - accuracy: 0.7345 - val_loss: 0.6712 - val_accuracy: 0.6552\n",
      "Epoch 60/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5456 - accuracy: 0.7080 - val_loss: 0.6699 - val_accuracy: 0.6897\n",
      "Epoch 61/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6983 - accuracy: 0.6991 - val_loss: 0.6283 - val_accuracy: 0.7241\n",
      "Epoch 62/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6041 - accuracy: 0.7699 - val_loss: 0.6509 - val_accuracy: 0.7241\n",
      "Epoch 63/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4981 - accuracy: 0.8053 - val_loss: 0.9407 - val_accuracy: 0.6897\n",
      "Epoch 64/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7226 - accuracy: 0.6991 - val_loss: 0.7598 - val_accuracy: 0.7586\n",
      "Epoch 65/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.6061 - accuracy: 0.7345 - val_loss: 0.6458 - val_accuracy: 0.6552\n",
      "Epoch 66/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5253 - accuracy: 0.7345 - val_loss: 0.6206 - val_accuracy: 0.7931\n",
      "Epoch 67/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5636 - accuracy: 0.7788 - val_loss: 0.6967 - val_accuracy: 0.6552\n",
      "Epoch 68/1000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.5247 - accuracy: 0.7257 - val_loss: 0.7891 - val_accuracy: 0.7931\n",
      "Epoch 69/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6272 - accuracy: 0.7080 - val_loss: 0.9370 - val_accuracy: 0.6897\n",
      "Epoch 70/1000\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.6944 - accuracy: 0.7168 - val_loss: 1.1165 - val_accuracy: 0.4828\n",
      "Epoch 71/1000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 1.0608 - accuracy: 0.5575 - val_loss: 0.6388 - val_accuracy: 0.6897\n",
      "Epoch 72/1000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.6720 - accuracy: 0.7168 - val_loss: 0.9998 - val_accuracy: 0.6552\n",
      "Epoch 73/1000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.7801 - accuracy: 0.7080 - val_loss: 1.3197 - val_accuracy: 0.4828\n",
      "Epoch 74/1000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 1.1770 - accuracy: 0.5310 - val_loss: 0.6631 - val_accuracy: 0.6552\n",
      "Epoch 75/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5521 - accuracy: 0.7080 - val_loss: 0.8582 - val_accuracy: 0.6552\n",
      "Epoch 76/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.6882 - accuracy: 0.7080 - val_loss: 0.6276 - val_accuracy: 0.7931\n",
      "Epoch 77/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.4830 - accuracy: 0.8319 - val_loss: 0.7197 - val_accuracy: 0.6552\n",
      "Epoch 78/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5508 - accuracy: 0.6991 - val_loss: 0.6626 - val_accuracy: 0.6552\n",
      "Epoch 79/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.5040 - accuracy: 0.7788 - val_loss: 1.0440 - val_accuracy: 0.4828\n",
      "Epoch 80/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.9209 - accuracy: 0.5575 - val_loss: 0.8656 - val_accuracy: 0.6552\n",
      "Epoch 81/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.8030 - accuracy: 0.6991 - val_loss: 0.7050 - val_accuracy: 0.6897\n",
      "Epoch 82/1000\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.8006 - accuracy: 0.6372 - val_loss: 0.7023 - val_accuracy: 0.6897\n",
      "Epoch 83/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.6811 - accuracy: 0.6991 - val_loss: 0.7645 - val_accuracy: 0.6897\n",
      "Epoch 84/1000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6013 - accuracy: 0.7876 - val_loss: 0.7034 - val_accuracy: 0.7241\n",
      "Epoch 85/1000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.5667 - accuracy: 0.7434 - val_loss: 0.7495 - val_accuracy: 0.6552\n",
      "Epoch 86/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5578 - accuracy: 0.7168 - val_loss: 0.6426 - val_accuracy: 0.8276\n",
      "Epoch 87/1000\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 0.5961 - accuracy: 0.7788 - val_loss: 0.8404 - val_accuracy: 0.6552\n",
      "Epoch 88/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.6371 - accuracy: 0.7257 - val_loss: 0.6490 - val_accuracy: 0.7241\n",
      "Epoch 89/1000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.4997 - accuracy: 0.7611 - val_loss: 0.7004 - val_accuracy: 0.7241\n",
      "Epoch 90/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5558 - accuracy: 0.7345 - val_loss: 0.6116 - val_accuracy: 0.7586\n",
      "Epoch 91/1000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4932 - accuracy: 0.7611 - val_loss: 0.8678 - val_accuracy: 0.6552\n",
      "Epoch 92/1000\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 0.6898 - accuracy: 0.6991 - val_loss: 0.6045 - val_accuracy: 0.7586\n",
      "Epoch 93/1000\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 0.5600 - accuracy: 0.7611 - val_loss: 0.8321 - val_accuracy: 0.6552\n",
      "Epoch 94/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6089 - accuracy: 0.6991 - val_loss: 0.6795 - val_accuracy: 0.6552\n",
      "Epoch 95/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.4673 - accuracy: 0.8319 - val_loss: 0.6238 - val_accuracy: 0.8276\n",
      "Epoch 96/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.5294 - accuracy: 0.7876 - val_loss: 0.8983 - val_accuracy: 0.6552\n",
      "Epoch 97/1000\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.7432 - accuracy: 0.6991 - val_loss: 0.6539 - val_accuracy: 0.7931\n",
      "Epoch 98/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.5638 - accuracy: 0.7788 - val_loss: 0.6658 - val_accuracy: 0.7241\n",
      "Epoch 99/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.4853 - accuracy: 0.8319 - val_loss: 0.7177 - val_accuracy: 0.6552\n",
      "Epoch 100/1000\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 0.4893 - accuracy: 0.7611 - val_loss: 0.8695 - val_accuracy: 0.5517\n",
      "Epoch 101/1000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.7598 - accuracy: 0.6549 - val_loss: 0.8813 - val_accuracy: 0.6552\n",
      "Epoch 102/1000\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 0.7301 - accuracy: 0.6991 - val_loss: 0.8469 - val_accuracy: 0.5517\n",
      "Epoch 103/1000\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.7115 - accuracy: 0.6814 - val_loss: 0.7185 - val_accuracy: 0.7241\n",
      "Epoch 104/1000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.5883 - accuracy: 0.7522 - val_loss: 0.6402 - val_accuracy: 0.7931\n",
      "Epoch 105/1000\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.5095 - accuracy: 0.8053 - val_loss: 0.7226 - val_accuracy: 0.6552\n",
      "Epoch 106/1000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6317 - accuracy: 0.6800Restoring model weights from the end of the best epoch.\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 0.6283 - accuracy: 0.6903 - val_loss: 0.6146 - val_accuracy: 0.6897\n",
      "Epoch 00106: early stopping\n",
      "걸린 시간 :  9.58253288269043\n"
     ]
    }
   ],
   "source": [
    "#3. 컴파일, 훈련\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=50, mode='min', verbose=1,\n",
    "                              restore_best_weights=True)\n",
    "\n",
    "start_time = time.time()\n",
    "hist = model.fit(x_train, y_train, epochs=1000, batch_size=100,\n",
    "                 validation_split=0.2, \n",
    "                 callbacks=[earlyStopping], verbose=1)\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('걸린 시간 : ', end_time)  # Epoch 00106: early stopping\n",
    "                                 # 걸린 시간 :  9.58253288269043S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc53902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 9ms/step - loss: 1.0800 - accuracy: 0.7222\n",
      "loss :  1.079993724822998\n",
      "acc :  0.7222222089767456\n",
      "정확도 :  0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print('loss : ', loss)\n",
    "print('acc : ', acc)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "y_predict = y_predict.argmax(axis=1)\n",
    "y_test = y_test.argmax(axis=1)\n",
    "\n",
    "acc = accuracy_score(y_test, y_predict)\n",
    "print('정확도 : ', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
